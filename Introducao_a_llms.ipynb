{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "262de988-08db-4f2b-85f9-061f63c8c70b",
      "metadata": {
        "id": "262de988-08db-4f2b-85f9-061f63c8c70b"
      },
      "source": [
        "<h1/>IntroduÃ§Ã£o a LLMs ğŸ¤–</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57a2a372-b6a9-4843-85d9-f135dc553005",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "57a2a372-b6a9-4843-85d9-f135dc553005"
      },
      "source": [
        "<h3/>O que sÃ£o LLMs?</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7669fdfe-b621-49d7-adbe-95493271e407",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "7669fdfe-b621-49d7-adbe-95493271e407"
      },
      "source": [
        "LLMs(Large Language Models) sÃ£o algoritmos de inteligÃªncia artificial, que aplicam tÃ©cnicas de rede neurais para realizaÃ§Ã£o de processamento da linguagem humana ou texto usando <a href=\"https://www.ibm.com/br-pt/think/topics/self-supervised-learning\"/>aprendizado autossupervisionado</a>. As principais tarefas que esses modelos realizam sÃ£o: GeraÃ§Ã£o de texto, traduÃ§Ã£o, resumos, geraÃ§Ã£o de imagem e chat-bots."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8550e86-9170-4248-8c2e-1067d435c50f",
      "metadata": {
        "id": "d8550e86-9170-4248-8c2e-1067d435c50f"
      },
      "source": [
        "A seguir uma imagem mostrando a evoluÃ§Ã£o do surgimento de LLMs no mercado:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae948f1e-7629-4173-a13f-e245990cb87f",
      "metadata": {
        "id": "ae948f1e-7629-4173-a13f-e245990cb87f"
      },
      "source": [
        "<img src=\"https://infohub.delltechnologies.com/static/media/13f83181-93ad-4024-a34b-26de431d4d17.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e46ccb50-ddac-4a3b-8eb5-273c757041fa",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "e46ccb50-ddac-4a3b-8eb5-273c757041fa"
      },
      "source": [
        "<h3>Como Funcionam?</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6582e15f-7e51-465f-a7b0-6618fd61b436",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "6582e15f-7e51-465f-a7b0-6618fd61b436"
      },
      "source": [
        "Basicamente, os LLMs funcionam com o principio do <a href=\"https://www.ibm.com/br-pt/think/topics/deep-learning\"/>Deep Learning</a>, fazendo uso das arquiteturas de redes neurais para processamento da linguagem humana. Geralmente esses modelos sÃ£o treinados com grandes <a href=\"https://www.ibm.com/br-pt/think/topics/dataset\"/>datasets</a> usando a tÃ©cnica de aprendizado autossupervisionado, a base de suas funcionalidades irÃ¡ variar com o relacionamento e padrÃµes linguÃ­sticos presentes nos dados usados para seu treinamento. A arquitetura de um LLM consiste em vÃ¡rias camadas, as principais sendo as camadas de avanÃ§o(feedfoward layers), camadas de incorporaÃ§Ã£o(embeddings layers) e as camadas de atenÃ§Ã£o(attention layers)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93c974ff-dbaf-4e4c-8f28-83059346f5bf",
      "metadata": {
        "id": "93c974ff-dbaf-4e4c-8f28-83059346f5bf"
      },
      "source": [
        "A seguir uma imagem contendo um diagrama de como as camadas se comportam:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e3286a0-0129-40f8-a3ae-b69d87908d77",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "2e3286a0-0129-40f8-a3ae-b69d87908d77"
      },
      "source": [
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20230531140926/Transformer-python-(1).png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51aa39d5-ec2e-4de5-8775-9581990be8a2",
      "metadata": {
        "id": "51aa39d5-ec2e-4de5-8775-9581990be8a2"
      },
      "source": [
        "Outros componentes que influenciam na arquitetura dos LLMs sÃ£o:\n",
        "<ul/>\n",
        "    <li>Tamanho do modelo e quantidade de <a href=\"https://pt.dataconomy.com/2025/05/08/parametros-llm/\">parÃ¢metros</a></li>\n",
        "    <li>RepresentaÃ§Ã£o do input</li>\n",
        "    <li>EficiÃªncia computacional</li>\n",
        "    <li>Mecanismos de auto-atenÃ§Ã£o</li>\n",
        "    <li>Objetivos de treino</li>\n",
        "    <li>DecodificaÃ§Ã£o e geraÃ§Ã£o de output</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f576a4e-d3d8-4f76-8d72-b1d7614b5734",
      "metadata": {
        "id": "6f576a4e-d3d8-4f76-8d72-b1d7614b5734"
      },
      "source": [
        "<h3>AplicaÃ§Ã£o dos LLMs</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecd09d68-f74f-4be3-9aa3-972a0096cf3f",
      "metadata": {
        "id": "ecd09d68-f74f-4be3-9aa3-972a0096cf3f"
      },
      "source": [
        "Atualmente os LLMs exercem as seguintes tarefas:\n",
        "<ul>\n",
        "    <li>\n",
        "    <b/>GeraÃ§Ã£o de CÃ³digo:</b> Os LLMs conseguem gerar cÃ³digos basedos na instruÃ§Ã£o do usuÃ¡rio para tarefas especÃ­ficas.\n",
        "    </li>\n",
        "    <li>\n",
        "    <b/>Debugging e DocumentaÃ§Ã£o:</b> Os mesmos podem ser utilizados para indentificaÃ§Ã£o de erros em cÃ³digo e automatizaÃ§Ã£o de documentaÃ§Ãµes de projetos.\n",
        "    </li>\n",
        "    <li>\n",
        "    <b/>Responder Perguntas:</b> O usuÃ¡rio pode realizar perguntas simples ou complexas, gerando respostas com contextos concisos.\n",
        "    </li>\n",
        "    <li>\n",
        "    <b/>TraduÃ§Ã£o e CorreÃ§Ã£o LiguÃ­stica:</b> Esses modelos tambÃ©m realizaÃ§Ã£o traduÃ§Ãµes e correÃ§Ãµes gramaticais na maioria das linguas.\n",
        "    </li>   \n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee818d72-0e35-4e13-b0f9-da503c106797",
      "metadata": {
        "id": "ee818d72-0e35-4e13-b0f9-da503c106797"
      },
      "source": [
        "Atualmente, existem diversas aplicaÃ§Ãµes no mercado que fazem uso de Modelos de Linguagem de Grande Escala (LLMs). A OpenAI Ã© uma das mais conhecidas, com o ChatGPT, mas nÃ£o Ã© a Ãºnica a oferecer soluÃ§Ãµes em larga escala baseadas nessa tecnologia. Outras empresas tambÃ©m tÃªm se destacado, como o Google, com o Gemini; a DeepSeek; e a Microsoft, com o Copilot. AlÃ©m disso, hÃ¡ aplicaÃ§Ãµes voltadas para propÃ³sitos especÃ­ficos, como o GitHub Copilot, focado em programaÃ§Ã£o, e o DALLÂ·E, da prÃ³pria OpenAI, voltado para geraÃ§Ã£o de imagens. Devido Ã  sua natureza adaptativa, a tendÃªncia Ã© que o uso de LLMs continue crescendo de forma significativa nos mais diversos setores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a71eca-19ec-4875-9fb4-877673189bf3",
      "metadata": {
        "id": "74a71eca-19ec-4875-9fb4-877673189bf3"
      },
      "source": [
        "<h3>Exemplo PrÃ¡tico</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58effd84-d58d-476b-a345-c082e74f86d8",
      "metadata": {
        "id": "58effd84-d58d-476b-a345-c082e74f86d8"
      },
      "source": [
        "Nesse exemplo usaremos a biblioteca <a href=\"https://huggingface.co/transformers/v3.0.2/index.html\">Transformers</a> do <a href=\"https://huggingface.co/\">Hugginface</a> que Ã© uma plataforma colaborativa voltada pra IA, semelhante ao GitHub, nele temos uma vasta coleÃ§Ã£o de modelos, datasets, entre outras informaÃ§Ãµes sobre IA. para iniciarmos precisamos configurar o ambiente e instalar as dependÃªncias nescessÃ¡rias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "20c78575-aeec-4cf8-837d-d2580ef8ced5",
      "metadata": {
        "scrolled": true,
        "id": "20c78575-aeec-4cf8-837d-d2580ef8ced5",
        "outputId": "7c8142ac-6fd8-43ea-b3c1-1918049844d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86f6443c-7f06-406a-a46f-2633737a4852",
      "metadata": {
        "id": "86f6443c-7f06-406a-a46f-2633737a4852"
      },
      "source": [
        "TambÃ©m Ã© necessÃ¡rio a instalaÃ§Ã£o do pytorch, a mesma Ã© uma biblioteca pra dessenvolvimento de IA, sendo uma das depÃªndencias para rodar os LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5d685a07-f416-4e5d-bb77-c213b5e69c00",
      "metadata": {
        "scrolled": true,
        "id": "5d685a07-f416-4e5d-bb77-c213b5e69c00",
        "outputId": "9c3e1e09-e6ce-4966-8e08-b6467f5f8d78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01cdbb6-2fb6-4a8c-9679-26132133e29c",
      "metadata": {
        "id": "a01cdbb6-2fb6-4a8c-9679-26132133e29c"
      },
      "source": [
        "Usaremos o GPT2 como exemplo para geraÃ§Ã£o de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "53ffc52c-23ee-40e4-a6f6-cc4fed1b4436",
      "metadata": {
        "id": "53ffc52c-23ee-40e4-a6f6-cc4fed1b4436",
        "outputId": "0c8aebc6-2ac9-4370-f8fb-198ba7353516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Hello, I\\'m a language model, but what I\\'m really trying to do is make everything a language. And this is where I\\'m really happy.\"\\n\\nThe language model is similar to the \"code\" model in Python. It involves only a small number of variables and does not involve any writing code. The language model, in contrast, enables you to write code that is easy to understand, understandable and fast to program.\\n\\nThe language model can be divided into three parts. The first one is to implement the concepts of the language model in Python and implement the language model in C. This is an important step to understand the language model, but it is not the only step.\\n\\nThe second piece is to make the compiler and the compiler-environment in Python more robust and easy to understand.\\n\\nThe third part is to implement the language model in C and to make the compiler and the compiler-environment in Python more robust and easy to understand.\\n\\nThe final part is to use the language model to make the compiler in Python have more features, so that the compiler can be used to build and make the compiler more robust.\\n\\nI hope you have found this post helpful and that your experiences with the language model in Python have helped you to make the decision to start using Python'},\n",
              " {'generated_text': \"Hello, I'm a language model, not a syntax model. I never got the chance to learn this idea. I've never been taught it in school, even in the '90s when we were very young, and I always thought it would be strange if you were doing it in a language you're familiar with. But I have a lot of friends who've been doing this, and I'm a bit surprised, because I don't think I've ever been taught a language that has this kind of experience, with the ability to do this with a language.\\n\\nI think it's kind of fun to imagine that you could have a language that you can learn and be an expert at. That's what I'm most excited about. I mean, I'm a big fan of some of the great writers and designers of the past 15 years, and I really like a lot of the great programmers. But I also think that it's really exciting to imagine a language that could be used, and then come out with something in the future that you can use in a very specific way.\\n\\nJUAN GONZÃLEZ: I'd like to ask you about the first time you started building a language. How did you start?\\n\\nJUAN GONZÃ\"},\n",
              " {'generated_text': \"Hello, I'm a language model, and I've been working on this for a while. I am going to show you how to create a simple API to connect web apps with JSON in Haskell.\\n\\nLet's talk about the basics.\\n\\nA web app\\n\\nA web app is a web-based application which is used to run a web server on a device, such as a laptop.\\n\\nThe web application is a file that is opened on your PC and is accessed by the web server.\\n\\nIn general, a web application is a file that is opened on your PC that is stored in memory inside a database. A database is a directory of files.\\n\\nIn Haskell, it is the directory where the code of the web application can be located.\\n\\nLet's go over what is a web application.\\n\\nThe web application is a file that is opened on your PC.\\n\\nLet's say you are reading a newspaper and you are browsing it.\\n\\nYou are reading this newspaper.\\n\\nHow does a web application interact with database?\\n\\nLet's suppose you have a computer that is running Windows 10.\\n\\nIt can read the news, view the news, download the news, and open that news file.\\n\\nBut, the computer is\"},\n",
              " {'generated_text': 'Hello, I\\'m a language model, not a system.\"\\n\\n\"But, my code is simple.\"\\n\\n\"Yes, but you\\'re not the only one. I have a lot of other people who come and talk to me and they talk to me, and they make me more interested in code, and I\\'m interested in learning more, and I\\'m interested in learning more, and I\\'m interested in learning more.\"\\n\\n\"But, why do you think that this is the case?\"\\n\\n\"Because I don\\'t like to do things that are not in my interest.\"\\n\\n\"Then why do you think it\\'s the case?\"\\n\\n\"Because I don\\'t like to be in any conflict with people who are not happy with me. I don\\'t like to have to compromise.\"\\n\\n\"But, you don\\'t like conflict?\"\\n\\n\"I don\\'t mind conflict, but I don\\'t like having conflicts with people who are happy with me.\"\\n\\n\"So, why do you think it\\'s the case?\"\\n\\n\"Because I\\'m not a good programmer, so I don\\'t like being in conflict with people who don\\'t like me.\"\\n\\n\"So, what\\'s your problem?\"\\n\\n\"I\\'m just not the right type of programmer.\"\\n'},\n",
              " {'generated_text': 'Hello, I\\'m a language model, I\\'m not a language model. I\\'m a language model.\"\\n\\n\"What? What is a language model?\"\\n\\n\"A language model is a set of objects that describe a particular human phenomenon. The language model is an important part of this understanding.\"\\n\\n\"But you don\\'t understand a language model?\"\\n\\n\"Well, I don\\'t understand a language model, but I understand a language model that can represent humans in terms of things that they\\'re comfortable with.\"\\n\\n\"You mean something like that?\"\\n\\n\"Yes, that\\'s correct. I mean I can talk and write and see and be like, how can you not understand that?\"\\n\\n\"But you didn\\'t understand a language model?\"\\n\\n\"No, that\\'s not the case. We\\'re talking about the human beings that are comfortable with things that they\\'re comfortable with. That\\'s the human beings that we\\'re talking about.\"\\n\\n\"We\\'re talking about the human beings that you\\'re talking about, and that\\'s the human beings you\\'re talking about?\"\\n\\n\"We\\'re talking about the humans that we\\'re talking about, and that\\'s the human beings that you\\'re talking about.\"\\n\\n\"And you\\'re talking about the human beings that you\\'re'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "import torch\n",
        "import os\n",
        "\n",
        "#A Classe pipeline abstrai conceitos, facilitando a conexÃ£o direta com huggin face e utilizaÃ§Ã£o dos LLMs.\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "#Essa inicializa um gerador de nÃºmeros pseudoaleatÃ³rios.\n",
        "set_seed(42)\n",
        "\n",
        "#Aqui definimos o texto base, o cumprimento mÃ¡ximo do texto que serÃ¡ gerado e o numero de retornos.\n",
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993aac26-8563-4642-aafa-c9d1b1bbbf70",
      "metadata": {
        "id": "993aac26-8563-4642-aafa-c9d1b1bbbf70"
      },
      "source": [
        "Isso Ã© um exemplo bÃ¡sico de utilizaÃ§Ã£o, podendo ser adaptado para diversas tarefas e casos de uso por meio de fine tuning, porÃ©m isso requer uso de um bom hardware e principalmente uma boa GPU, para realizaÃ§Ã£o de treinamentos mais especÃ­ficos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4d68ab1-2c86-473d-b42a-dc3079b62f44",
      "metadata": {
        "id": "b4d68ab1-2c86-473d-b42a-dc3079b62f44"
      },
      "source": [
        "<h3>BenefÃ­cios dos LLM</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ac7d64e-9c19-4793-854c-ce27369a6d69",
      "metadata": {
        "id": "9ac7d64e-9c19-4793-854c-ce27369a6d69"
      },
      "source": [
        "Os LLMs (Modelos de Linguagem de Grande Escala) sÃ£o capazes de realizar o <a href=\"https://www.ibm.com/br-pt/think/topics/zero-shot-learning\">aprendizado zero-shot</a>. Isso significa que conseguem generalizar tarefas para as quais nÃ£o foram explicitamente treinados, permitindo que se adaptem a novos cenÃ¡rios sem a necessidade de treinamento adicional.\n",
        "Outro benefÃ­cio importante Ã© a capacidade de manipular e compreender grandes volumes de dados, o que se aplica, por exemplo, Ã  traduÃ§Ã£o de idiomas e Ã  geraÃ§Ã£o de resumos de documentos.\n",
        "AlÃ©m disso, os LLMs podem passar por <a href=\"http://ibm.com/br-pt/think/topics/fine-tuning\">fine-tuning</a> com conjuntos de dados especÃ­ficos, o que permite que se mantenham em constante evoluÃ§Ã£o e adaptaÃ§Ã£o a diferentes casos de uso na indÃºstria.\n",
        "Por fim, esses modelos automatizam uma ampla variedade de tarefas, como geraÃ§Ã£o de cÃ³digo e criaÃ§Ã£o de conteÃºdo, otimizando tempo e reduzindo a necessidade de mÃ£o de obra em atividades repetitivas, permitindo que os recursos humanos sejam direcionados a demandas mais estratÃ©gicas e especÃ­ficas dentro de um projeto."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74087c1d-ba31-4c87-90c8-9417b82d299d",
      "metadata": {
        "id": "74087c1d-ba31-4c87-90c8-9417b82d299d"
      },
      "source": [
        "<h3>ConclusÃ£o</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1aaa83-1d9f-4ac3-a0c8-15b7461fa8ed",
      "metadata": {
        "id": "7b1aaa83-1d9f-4ac3-a0c8-15b7461fa8ed"
      },
      "source": [
        "Diante dos tÃ³picos abordados, Ã© possÃ­vel afirmar com seguranÃ§a que os LLMs (Modelos de Linguagem de Grande Escala) jÃ¡ se tornaram uma parte fundamental dos avanÃ§os tecnolÃ³gicos na indÃºstria. Sua presenÃ§a Ã© cada vez mais evidente em soluÃ§Ãµes inovadoras, integrando-se a fluxos de trabalho, produtos e serviÃ§os de maneira estratÃ©gica.\n",
        "A tendÃªncia Ã© de avanÃ§os contÃ­nuos e cada vez mais expressivos nessa tecnologia. Praticamente todas as big techs â€” como Google, Microsoft, Meta, Amazon e OpenAI â€” jÃ¡ desenvolvem e oferecem soluÃ§Ãµes baseadas em LLMs, investindo pesadamente em pesquisa e desenvolvimento para ampliar a capacidade e eficiÃªncia desses modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce212121-0427-437d-98f2-5f3645c68c5b",
      "metadata": {
        "id": "ce212121-0427-437d-98f2-5f3645c68c5b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}